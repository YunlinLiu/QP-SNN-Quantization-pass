================================================================================
BERT模型量化前后对比
================================================================================

【原始BERT模型结构】
--------------------------------------------------
RobertaForSequenceClassification(
  (roberta): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0-11): 12 x RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttentionLSQInteger(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (query_quan): LSQInteger(level=32, sym=True, pos_max=15.0, neg_min=-16.0, s=1.0)
              (key_quan): LSQInteger(level=32, sym=True, pos_max=15.0, neg_min=-16.0, s=1.0)
              (value_quan): LSQInteger(level=32, sym=True, pos_max=15.0, neg_min=-16.0, s=1.0)
              (attn_quan): LSQInteger(level=32, sym=False, pos_max=31.0, neg_min=0.0, s=1.0)
              (after_attn_quan): LSQInteger(level=32, sym=False, pos_max=31.0, neg_min=0.0, s=1.0)
            )
            (output): RobertaSelfOutputLSQInteger(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (dense_quan): LSQInteger(level=32, sym=True, pos_max=15.0, neg_min=-16.0, s=1.0)
              (dense_quan_after_ln): LSQInteger(level=32, sym=True, pos_max=15.0, neg_min=-16.0, s=1.0)
            )
          )
          (intermediate): RobertaIntermediateLSQInteger(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): ReLU()
            (dense_quan): LSQInteger(level=32, sym=False, pos_max=31.0, neg_min=0.0, s=1.0)
          )
          (output): RobertaOutputLSQInteger(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (dense_quan): LSQInteger(level=32, sym=True, pos_max=15.0, neg_min=-16.0, s=1.0)
            (dense_quan_after_ln): LSQInteger(level=32, sym=True, pos_max=15.0, neg_min=-16.0, s=1.0)
          )
        )
      )
    )
  )
  (classifier): RobertaClassificationHeadLSQInteger(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (out_proj): Linear(in_features=768, out_features=2, bias=True)
    (dense_quan): LSQInteger(level=32, sym=False, pos_max=31.0, neg_min=0.0, s=1.0)
  )
)

【量化后BERT模型结构】
--------------------------------------------------
RobertaForSequenceClassification(
  (roberta): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0-11): 12 x RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttentionLSQInteger(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (query_quan): LSQInteger(level=32, sym=True, pos_max=15.0, neg_min=-16.0, s=1.0)
              (key_quan): LSQInteger(level=32, sym=True, pos_max=15.0, neg_min=-16.0, s=1.0)
              (value_quan): LSQInteger(level=32, sym=True, pos_max=15.0, neg_min=-16.0, s=1.0)
              (attn_quan): LSQInteger(level=32, sym=False, pos_max=31.0, neg_min=0.0, s=1.0)
              (after_attn_quan): LSQInteger(level=32, sym=False, pos_max=31.0, neg_min=0.0, s=1.0)
            )
            (output): RobertaSelfOutputLSQInteger(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (dense_quan): LSQInteger(level=32, sym=True, pos_max=15.0, neg_min=-16.0, s=1.0)
              (dense_quan_after_ln): LSQInteger(level=32, sym=True, pos_max=15.0, neg_min=-16.0, s=1.0)
            )
          )
          (intermediate): RobertaIntermediateLSQInteger(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): ReLU()
            (dense_quan): LSQInteger(level=32, sym=False, pos_max=31.0, neg_min=0.0, s=1.0)
          )
          (output): RobertaOutputLSQInteger(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (dense_quan): LSQInteger(level=32, sym=True, pos_max=15.0, neg_min=-16.0, s=1.0)
            (dense_quan_after_ln): LSQInteger(level=32, sym=True, pos_max=15.0, neg_min=-16.0, s=1.0)
          )
        )
      )
    )
  )
  (classifier): RobertaClassificationHeadLSQInteger(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (out_proj): Linear(in_features=768, out_features=2, bias=True)
    (dense_quan): LSQInteger(level=32, sym=False, pos_max=31.0, neg_min=0.0, s=1.0)
  )
)

【主要变化总结】
--------------------------------------------------
1. RobertaSdpaSelfAttention → RobertaSelfAttentionLSQInteger
2. RobertaSelfOutput → RobertaSelfOutputLSQInteger
3. RobertaIntermediate → RobertaIntermediateLSQInteger
4. RobertaOutput → RobertaOutputLSQInteger
5. RobertaClassificationHead → RobertaClassificationHeadLSQInteger

每个量化模块都添加了LSQInteger量化器，level=32位
