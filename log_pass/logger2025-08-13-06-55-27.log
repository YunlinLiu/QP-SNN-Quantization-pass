08/13 06:55:27 AM | args = Namespace(arch='vgg_16_bn', job_dir='./log_pass/', batch_size=256, epochs=10, lr=0.001, resume=False, gpu='0', dataset='CIFAR10', workers=8, bit=8)
08/13 06:55:28 AM | ==> Building model..
08/13 06:55:28 AM | === Bit width===:8
08/13 06:55:28 AM | ==> Creating model via pass-based quantization..
08/13 06:55:30 AM | ==> Pass-based quantization completed
08/13 06:55:30 AM | VGG(
  (features): Sequential(
    (convbn0): tdLayer(
      (layer): SeqToANNContainer(
        (module): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (bn): tdBatchNorm(
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (seqbn): SeqToANNContainer(
          (module): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (relu0): LIFSpike()
    (convbn1): tdLayer(
      (layer): SeqToANNContainer(
        (module): Conv2dReScaW(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (weight_quan): ReScaW()
        )
      )
      (bn): tdBatchNorm(
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (seqbn): SeqToANNContainer(
          (module): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (relu1): LIFSpike()
    (pool2): SeqToANNContainer(
      (module): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (convbn3): tdLayer(
      (layer): SeqToANNContainer(
        (module): Conv2dReScaW(
          64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (weight_quan): ReScaW()
        )
      )
      (bn): tdBatchNorm(
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (seqbn): SeqToANNContainer(
          (module): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (relu3): LIFSpike()
    (convbn4): tdLayer(
      (layer): SeqToANNContainer(
        (module): Conv2dReScaW(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (weight_quan): ReScaW()
        )
      )
      (bn): tdBatchNorm(
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (seqbn): SeqToANNContainer(
          (module): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (relu4): LIFSpike()
    (pool5): SeqToANNContainer(
      (module): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (convbn6): tdLayer(
      (layer): SeqToANNContainer(
        (module): Conv2dReScaW(
          128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (weight_quan): ReScaW()
        )
      )
      (bn): tdBatchNorm(
        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (seqbn): SeqToANNContainer(
          (module): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (relu6): LIFSpike()
    (convbn7): tdLayer(
      (layer): SeqToANNContainer(
        (module): Conv2dReScaW(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (weight_quan): ReScaW()
        )
      )
      (bn): tdBatchNorm(
        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (seqbn): SeqToANNContainer(
          (module): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (relu7): LIFSpike()
    (convbn8): tdLayer(
      (layer): SeqToANNContainer(
        (module): Conv2dReScaW(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (weight_quan): ReScaW()
        )
      )
      (bn): tdBatchNorm(
        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (seqbn): SeqToANNContainer(
          (module): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (relu8): LIFSpike()
    (pool9): SeqToANNContainer(
      (module): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (convbn10): tdLayer(
      (layer): SeqToANNContainer(
        (module): Conv2dReScaW(
          256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (weight_quan): ReScaW()
        )
      )
      (bn): tdBatchNorm(
        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (seqbn): SeqToANNContainer(
          (module): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (relu10): LIFSpike()
    (convbn11): tdLayer(
      (layer): SeqToANNContainer(
        (module): Conv2dReScaW(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (weight_quan): ReScaW()
        )
      )
      (bn): tdBatchNorm(
        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (seqbn): SeqToANNContainer(
          (module): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (relu11): LIFSpike()
    (convbn12): tdLayer(
      (layer): SeqToANNContainer(
        (module): Conv2dReScaW(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (weight_quan): ReScaW()
        )
      )
      (bn): tdBatchNorm(
        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (seqbn): SeqToANNContainer(
          (module): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (relu12): LIFSpike()
    (pool13): SeqToANNContainer(
      (module): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (convbn14): tdLayer(
      (layer): SeqToANNContainer(
        (module): Conv2dReScaW(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (weight_quan): ReScaW()
        )
      )
      (bn): tdBatchNorm(
        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (seqbn): SeqToANNContainer(
          (module): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (relu14): LIFSpike()
    (convbn15): tdLayer(
      (layer): SeqToANNContainer(
        (module): Conv2dReScaW(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (weight_quan): ReScaW()
        )
      )
      (bn): tdBatchNorm(
        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (seqbn): SeqToANNContainer(
          (module): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (relu15): LIFSpike()
    (convbn16): tdLayer(
      (layer): SeqToANNContainer(
        (module): Conv2dReScaW(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (weight_quan): ReScaW()
        )
      )
      (bn): tdBatchNorm(
        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (seqbn): SeqToANNContainer(
          (module): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (relu16): LIFSpike()
  )
  (avgpool): SeqToANNContainer(
    (module): AvgPool2d(kernel_size=2, stride=2, padding=0)
  )
  (classifier): Sequential(
    (linear1): SeqToANNContainer(
      (module): Linear(in_features=512, out_features=10, bias=True)
    )
  )
)
08/13 06:55:30 AM | training from scratch
08/13 06:55:30 AM | learning_rate: 0.001
08/13 06:55:32 AM | Epoch[0](0/196): Loss 2.3044 Prec@1(1) 10.55
08/13 06:55:37 AM | Epoch[0](50/196): Loss 2.1948 Prec@1(1) 15.77
08/13 06:55:42 AM | Epoch[0](100/196): Loss 2.1080 Prec@1(1) 18.66
08/13 06:55:47 AM | Epoch[0](150/196): Loss 2.0520 Prec@1(1) 20.63
08/13 06:55:54 AM |  * Acc@1 34.570
08/13 06:55:56 AM | =>Best accuracy 34.570
08/13 06:55:56 AM | learning_rate: 0.0009755282581475768
08/13 06:55:57 AM | Epoch[1](0/196): Loss 1.8311 Prec@1(1) 28.12
08/13 06:56:02 AM | Epoch[1](50/196): Loss 1.8610 Prec@1(1) 28.99
08/13 06:56:07 AM | Epoch[1](100/196): Loss 1.8448 Prec@1(1) 29.94
08/13 06:56:12 AM | Epoch[1](150/196): Loss 1.8312 Prec@1(1) 30.83
08/13 06:56:18 AM |  * Acc@1 44.470
08/13 06:56:20 AM | =>Best accuracy 44.470
08/13 06:56:20 AM | learning_rate: 0.0009045084971874736
08/13 06:56:21 AM | Epoch[2](0/196): Loss 1.7271 Prec@1(1) 32.42
08/13 06:56:26 AM | Epoch[2](50/196): Loss 1.6720 Prec@1(1) 37.39
08/13 06:56:31 AM | Epoch[2](100/196): Loss 1.6485 Prec@1(1) 38.79
08/13 06:56:36 AM | Epoch[2](150/196): Loss 1.6341 Prec@1(1) 39.47
08/13 06:56:42 AM |  * Acc@1 51.200
08/13 06:56:44 AM | =>Best accuracy 51.200
08/13 06:56:44 AM | learning_rate: 0.0007938926261462366
08/13 06:56:45 AM | Epoch[3](0/196): Loss 1.5725 Prec@1(1) 44.53
08/13 06:56:50 AM | Epoch[3](50/196): Loss 1.5237 Prec@1(1) 45.00
08/13 06:56:55 AM | Epoch[3](100/196): Loss 1.5012 Prec@1(1) 45.37
08/13 06:57:00 AM | Epoch[3](150/196): Loss 1.4928 Prec@1(1) 45.61
08/13 06:57:07 AM |  * Acc@1 55.950
08/13 06:57:08 AM | =>Best accuracy 55.950
08/13 06:57:08 AM | learning_rate: 0.0006545084971874737
08/13 06:57:09 AM | Epoch[4](0/196): Loss 1.5054 Prec@1(1) 45.70
08/13 06:57:14 AM | Epoch[4](50/196): Loss 1.4066 Prec@1(1) 49.13
08/13 06:57:19 AM | Epoch[4](100/196): Loss 1.3924 Prec@1(1) 49.80
08/13 06:57:24 AM | Epoch[4](150/196): Loss 1.3870 Prec@1(1) 49.83
08/13 06:57:31 AM |  * Acc@1 63.830
08/13 06:57:32 AM | =>Best accuracy 63.830
08/13 06:57:32 AM | learning_rate: 0.0005
08/13 06:57:33 AM | Epoch[5](0/196): Loss 1.3586 Prec@1(1) 50.00
08/13 06:57:38 AM | Epoch[5](50/196): Loss 1.3104 Prec@1(1) 52.67
08/13 06:57:43 AM | Epoch[5](100/196): Loss 1.3005 Prec@1(1) 53.33
08/13 06:57:48 AM | Epoch[5](150/196): Loss 1.2958 Prec@1(1) 53.46
08/13 06:57:55 AM |  * Acc@1 63.690
08/13 06:57:55 AM | =>Best accuracy 63.830
08/13 06:57:55 AM | learning_rate: 0.00034549150281252633
08/13 06:57:56 AM | Epoch[6](0/196): Loss 1.3261 Prec@1(1) 50.00
08/13 06:58:01 AM | Epoch[6](50/196): Loss 1.2496 Prec@1(1) 54.80
08/13 06:58:06 AM | Epoch[6](100/196): Loss 1.2348 Prec@1(1) 55.45
08/13 06:58:11 AM | Epoch[6](150/196): Loss 1.2281 Prec@1(1) 55.88
08/13 06:58:18 AM |  * Acc@1 66.600
08/13 06:58:19 AM | =>Best accuracy 66.600
08/13 06:58:19 AM | learning_rate: 0.0002061073738537635
08/13 06:58:20 AM | Epoch[7](0/196): Loss 1.0801 Prec@1(1) 64.06
08/13 06:58:25 AM | Epoch[7](50/196): Loss 1.1607 Prec@1(1) 58.30
08/13 06:58:30 AM | Epoch[7](100/196): Loss 1.1615 Prec@1(1) 58.40
08/13 06:58:35 AM | Epoch[7](150/196): Loss 1.1577 Prec@1(1) 58.63
08/13 06:58:42 AM |  * Acc@1 70.600
08/13 06:58:43 AM | =>Best accuracy 70.600
08/13 06:58:43 AM | learning_rate: 9.549150281252634e-05
08/13 06:58:44 AM | Epoch[8](0/196): Loss 1.0933 Prec@1(1) 64.45
08/13 06:58:49 AM | Epoch[8](50/196): Loss 1.1330 Prec@1(1) 59.19
08/13 06:58:54 AM | Epoch[8](100/196): Loss 1.1264 Prec@1(1) 59.61
08/13 06:58:59 AM | Epoch[8](150/196): Loss 1.1223 Prec@1(1) 59.69
08/13 06:59:06 AM |  * Acc@1 72.020
08/13 06:59:07 AM | =>Best accuracy 72.020
08/13 06:59:07 AM | learning_rate: 2.4471741852423235e-05
08/13 06:59:08 AM | Epoch[9](0/196): Loss 1.0814 Prec@1(1) 60.94
08/13 06:59:13 AM | Epoch[9](50/196): Loss 1.1056 Prec@1(1) 60.89
08/13 06:59:18 AM | Epoch[9](100/196): Loss 1.0973 Prec@1(1) 60.99
08/13 06:59:23 AM | Epoch[9](150/196): Loss 1.0956 Prec@1(1) 60.88
08/13 06:59:30 AM |  * Acc@1 72.640
08/13 06:59:31 AM | =>Best accuracy 72.640
