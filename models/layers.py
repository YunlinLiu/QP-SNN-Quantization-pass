import torch
import torch.nn as nn


class SeqToANNContainer(nn.Module):
    # This code is form spikingjelly https://github.com/fangwei123456/spikingjelly
    def __init__(self, *args):
        super().__init__()
        if len(args) == 1:
            self.module = args[0]
        else:
            self.module = nn.Sequential(*args)

    def forward(self, x_seq: torch.Tensor): # TBCHW
        y_shape = [x_seq.shape[0], x_seq.shape[1]]  #T*B,C,H,W
        y_seq = self.module(x_seq.flatten(0, 1).contiguous())
        y_shape.extend(y_seq.shape[1:])
        return y_seq.view(y_shape)


class ClassifyLinear(nn.Module):

    def __init__(self, linear, ):
        super(ClassifyLinear, self).__init__()
        self.ops = linear

    def forward(self, x):
        step = x.size(1)
        out = []
        for i in range(step):
            out += [self.ops(x[:,i,:])]
        out = torch.stack(out,dim=1)
        return out

class Layer(nn.Module):
    def __init__(self,in_plane,out_plane,kernel_size,stride,padding):
        super(Layer, self).__init__()
        self.fwd = SeqToANNContainer(
            nn.Conv2d(in_plane,out_plane,kernel_size,stride,padding),
            nn.BatchNorm2d(out_plane)
        )
        self.act = LIFSpike()

    def forward(self,x):
        x = self.fwd(x)
        x = self.act(x)
        return x


class ZIF(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input, gama):
        out = (input > 0).float()
        L = torch.tensor([gama])
        ctx.save_for_backward(input, out, L)
        return out

    @staticmethod
    def backward(ctx, grad_output):
        (input, out, others) = ctx.saved_tensors
        gama = others[0].item()
        grad_input = grad_output.clone()
        tmp = (1 / gama) * (1 / gama) * ((gama - input.abs()).clamp(min=0))
        grad_input = grad_input * tmp
        return grad_input, None

class LIFSpike(nn.Module):
    def __init__(self, thresh=1.0, tau=0.5, gama=1.0):
        super(LIFSpike, self).__init__()
        self.act = ZIF.apply    #ZIF = Zero If Functionï¼Œè¿™æ˜¯ä½œè€…è‡ªå·±å®ç°çš„è„‰å†²æ¿€æ´»å‡½æ•°ï¼
        # ZIF.applyæ˜¯PyTorchè°ƒç”¨è‡ªå®šä¹‰Functionçš„æ ‡å‡†æ–¹å¼
        self.thresh = thresh
        self.tau = tau
        self.gama = gama

    def forward(self, x):
        mem = 0#æ¯ä¸ªbatchçš„meméƒ½åˆå§‹åŒ–ä¸º0
        spike_pot = []#æ¯ä¸ªbatchçš„spike_potéƒ½åˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨
        T = x.shape[1]#Tæ˜¯æ—¶é—´æ­¥æ•°ï¼ŒT=4
        # ğŸ”¥ å…³é”®å¾ªç¯ï¼š4æ¬¡è†œç”µä½æ›´æ–°ï¼
        for t in range(T):
            if len(x.shape)==3:
                inp = x[:,t,:]
            else:
                # ç¬¬1æ­¥ï¼šå–ç¬¬tä¸ªæ—¶é—´æ­¥çš„è¾“å…¥
                inp = x[:,t,:,:,:] # (256, 64, 32, 32)
            # ç¬¬2æ­¥ï¼šæ›´æ–°è†œç”µä½
            mem = mem * self.tau + inp     
            # ç¬¬3æ­¥ï¼šè„‰å†²å‘æ”¾
            spike = self.act(mem - self.thresh, self.gama)
            # ç¬¬4æ­¥ï¼šè†œç”µä½é‡ç½®ï¼ˆç¡¬é‡ç½®ï¼‰
            mem = (1 - spike) * mem
            # ç¬¬5æ­¥ï¼šä¿å­˜è¿™ä¸ªæ—¶é—´æ­¥çš„è„‰å†²
            spike_pot.append(spike)# spikeå½¢çŠ¶: (256, 64, 32, 32)
             # ğŸ¯ åˆå¹¶æ‰€æœ‰æ—¶é—´æ­¥çš„è„‰å†²
        return torch.stack(spike_pot, dim=1)# (256, 4, 64, 32, 32)


def add_dimention(x, T):
    x.unsqueeze_(1) # åœ¨ç¬¬1ç»´æ’å…¥æ–°ç»´åº¦
    x = x.repeat(1, T, 1, 1, 1)# åœ¨æ—¶é—´ç»´åº¦ä¸Šé‡å¤Tæ¬¡
    return x
# è¾“å…¥ï¼šæ ‡å‡†çš„4ç»´å›¾åƒæ•°æ®
#nput_shape = (256, 3, 32, 32)  # (Batch, Channel, Height, Width)

# æ­¥éª¤1ï¼šunsqueeze_(1) åœ¨ç¬¬1ç»´æ’å…¥æ—¶é—´ç»´åº¦
#after_unsqueeze = (256, 1, 3, 32, 32)  # (B, T=1, C, H, W)

# æ­¥éª¤2ï¼šrepeat(1, T, 1, 1, 1) åœ¨æ—¶é—´ç»´åº¦é‡å¤
#final_shape = (256, 4, 3, 32, 32)      # (B, T=4, C, H, W)

# ----- For ResNet19 code -----


class tdLayer(nn.Module):
    def __init__(self, layer, bn=None):
        super(tdLayer, self).__init__()
        self.layer = SeqToANNContainer(layer)
        self.bn = bn

    def forward(self, x):
        x_ = self.layer(x)
        if self.bn is not None:
            x_ = self.bn(x_)
        return x_

class tdBatchNorm(nn.Module):
    def __init__(self, out_panel):
        super(tdBatchNorm, self).__init__()
        self.bn = nn.BatchNorm2d(out_panel)# æ ‡å‡†æ‰¹å½’ä¸€åŒ–
        self.seqbn = SeqToANNContainer(self.bn)# 2. åŒ…è£…æˆæ”¯æŒæ—¶é—´åºåˆ—çš„ç‰ˆæœ¬

    def forward(self, x):
        y = self.seqbn(x)
        return y

# # cla params
# class tdBatchNorm(nn.Module):
#     def __init__(self, out_panel):
#         super(tdBatchNorm, self).__init__()
#         self.seqbn = SeqToANNContainer(nn.BatchNorm2d(out_panel))
#
#     def forward(self, x):
#         y = self.seqbn(x)
#         return y

"""class myBatchNorm3d(nn.Module):
    def __init__(self, inplanes, step):
        super().__init__()
        self.bn = nn.BatchNorm3d(inplanes)
        self.step = step
    def forward(self, x):
        out = x.permute(1, 2, 0, 3, 4)
        out = self.bn(out)
        out = out.permute(2, 0, 1, 3, 4).contiguous()
        return out"""