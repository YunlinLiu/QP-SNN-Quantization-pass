Going Deeper With Directly-Trained Larger Spiking Neural Networks
Hanle Zheng 1
, Yujie Wu 1
, Lei Deng 1,3, Yifan Hu 1 and Guoqi Li 1,2 *
1 Center for Brain-Inspired Computing Research, Department of Precision Instrument, Tsinghua University, Beijing 100084,
China
2 Beijing Innovation Center for Future Chip, Tsinghua University, Beijing 100084, China
3 Department of Electrical and Computer Engineering, University of California, Santa Barbara
Abstract
Spiking neural networks (SNNs) are promising in a bioï¿¾plausible coding for spatio-temporal information and eventï¿¾driven signal processing, which is very suited for energyï¿¾efficient implementation in neuromorphic hardware. Howï¿¾ever, the unique working mode of SNNs makes them more
difficult to train than traditional networks. Currently, there are
two main routes to explore the training of deep SNNs with
high performance. The first is to convert a pre-trained ANN
model to its SNN version, which usually requires a long codï¿¾ing window for convergence and cannot exploit the spatioï¿¾temporal features during training for solving temporal tasks.
The other is to directly train SNNs in the spatio-temporal
domain. But due to the binary spike activity of the firing
function and the problem of gradient vanishing or exploï¿¾sion, current methods are restricted to shallow architectures
and thereby difficult in harnessing large-scale datasets (e.g.
ImageNet). To this end, we propose a threshold-dependent
batch normalization (tdBN) method based on the emerging
spatio-temporal backpropagation, termed â€œSTBP-tdBNâ€, enï¿¾abling direct training of a very deep SNN and the efficient
implementation of its inference on neuromorphic hardware.
With the proposed method and elaborated shortcut connecï¿¾tion, we significantly extend directly-trained SNNs from a
shallow structure (<10 layer) to a very deep structure (50 layï¿¾ers). Furthermore, we theoretically analyze the effectiveness
of our method based on â€œBlock Dynamical Isometryâ€ theory.
Finally, we report superior accuracy results including 93.15%
on CIFAR-10, 67.8% on DVS-CIFAR10, and 67.05% on Imï¿¾ageNet with very few timesteps. To our best knowledge, itâ€™s
the first time to explore the directly-trained deep SNNs with
high performance on ImageNet. We believe this work shall
pave the way of fully exploiting the advantages of SNNs and
attract more researchers to contribute in this field.
Introduction
Inspired by human neuronsâ€™ working patterns, spiking neuï¿¾ral networks (SNNs) have been considered as a promisï¿¾ing model in artificial intelligence and theoretical neuroï¿¾science (Roy, Jaiswal, and Panda 2019). Benefited from inï¿¾trinsic neuronal dynamics and event-driven spike communiï¿¾cation paradigms, SNNs show great potential in continuous
spatio-temporal information processing with lower energy
*Corresponding author: Guoqi Li (liguoqi@tsinghua.edu.cn)
Copyright Â© 2021, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.
consumption and better robustness(Stromatias et al. 2015).
Moreover, SNNs can be easily applied on some specialized
neuromorphic hardware (Merolla et al. 2014; Davies et al.
2018), which may be seen as the next generation of neural
networks.
There are two main approaches to train an SNN with
high performance. One is to convert a pre-trained ANN
to an SNN model, which usually needs hundreds of
timesteps(Sengupta et al. 2019; Hu et al. 2018). So, even
though these SNNs achieve comparable accuracy to ANNs
with similar structures, the large number of timesteps causes
serious signal latency and increase the amount of compuï¿¾tation. The other is to train SNNs directly based on graï¿¾dient descent method, which is independent of pre-trained
ANNs and can lessen the number of timesteps. A recent
work (Wu et al. 2018), which proposes a learning algorithm
called â€œSpatio-temporal backpropagationâ€ (STBP) with an
approach to training SNNs directly on ANN-oriented proï¿¾gramming frameworks (e.g. Pytorch), provides us with a
chance to explore deeper and larger directly-trained SNNs.
However, SNNs trained by this algorithm are restricted to
shallow architectures and cannot achieve satisfactory perforï¿¾mance on large-scale datasets such as ImageNet. So under
above algorithm, we clarify two problems to be solved for
training deeper SNNs directly.
The first problem is gradient vanishing or explosion. Beï¿¾cause of special mechanism of spatio-temporal information
processing and non-differentiable spiking signal, the gradiï¿¾ent propagation behaves much unstable and tends to vanï¿¾ish in most cases when training SNNs directly, which preï¿¾vents SNNs from going deeper. So far, there isnâ€™t an effecï¿¾tive method to handle this problem well in directly-trained
SNNs. Famous works (Lee et al. 2020; Lee, Delbruck, and
Pfeiffer 2016; Wu et al. 2019) before us fail to train deep
SNNs directly and all of their models are less than 10 layers,
which seriously influences the performances of their methï¿¾ods.
The other problem is that we need to balance the threshï¿¾old and input on each neuron to get appropriate firing rates
in SNNs. When the input is too small compared with the
threshold, the neuron fires few spikes and the neuronal memï¿¾brane potential remains so the information handled by the
neuron cannot be expressed enough. When the input is too
large, the neuron fires all the time and is insensitive to
arXiv:2011.05280v2 [cs.NE] 18 Dec 2020
the change of input. For directly-trained SNNs, with binary
spikes propagating layer by layer, the distribution of preï¿¾synaptic inputs will shift during the training process, making
the size of inputs inappropriate. Many methods have been
proposed to deal with it, such as threshold regularization
(Lee, Delbruck, and Pfeiffer 2016) and NeuNorm (Wu et al.
2019).
Normalization seems to be appropriate methods to solve
both of the problems. They stabilize the network and gradiï¿¾ent propagation according to (Chen et al. 2020). Also, they
normalize the distributions of pre-synaptic inputs to same
expectation and variance, which helps to balance the threshï¿¾old and input by reducing the internal covariate shift. Howï¿¾ever, the existing normalization methods arenâ€™t suitable for
training of SNNs. For the additional temporal dimension and
special activation mechanism, directly-trained SNNs need a
specially designed normalization method .
In this paper, we develop a new algorithm to train a deep
SNN directly. The main contributions of this work are sumï¿¾marized as follows:
â€¢ We propose the threshold-dependent batch normalization
to solve the gradient vanishing or explosion problem and
adjust firing rate. Furthermore, we take the residual netï¿¾work structure and modify the shortcut connections which
are suitable for deep SNNs.
â€¢ On this basis, we investigate very deep directly-trained
SNNs (extending them from less than 10 to 50 layï¿¾ers) and test them over large-scale non-spiking datasets
(CIFAR-10, ImageNet) and neuromorphic datasets (DVSï¿¾Gesture, DVS-CIFAR10).
â€¢ On CIFAR-10 and ImageNet, we comprehensively valï¿¾idate different SNN architectures (ResNet-18, 34, 50)
and report competitive results compared to similar SNNs
with much fewer timesteps (no more than 6 timesteps),
to our best knowledge, which is the first time that
the directly-trained SNN with full spikes report fairly
high accuracy on ImageNet. On neuromorphic datasets,
our model achieves state-of-the-art performance on both
DVS-Gesture and DVS-CIFAR10, which shows advanï¿¾tage of SNNs on dealing with temporal-spatial informaï¿¾tion.
Related Work
Learning algorithm of SNNs In the past few years, a
lot of learning algorithms have explored how to train a
deep SNN with high performance, including: (1) some apï¿¾proaches to converting pre-trained ANNs to SNNs; (2) graï¿¾dient descent based algorithms.
The first one is called as the â€œANN-SNN conversion methï¿¾odsâ€ (Sengupta et al. 2019; Han, Srinivasan, and Roy 2020),
seen as the most popular way to train deep SNNs with
high performance, which transforms the real-valued outï¿¾put of ReLU function to binary spikes in the SNN. This
kind of method successfully reports competitive results over
large-scale datasets without serious degradation compared to
ANNs. However, it ignores rich temporal dynamic behaviors
of spiking neurons and usually requires hundreds or thouï¿¾sands of time steps to approach the accuracy of pre-trained
ANNs.
The gradient descent based algorithms train SNNs with
error backpropagation. With gradient descent optimization
learning algorithms, some SNN models (Lee, Delbruck, and
Pfeiffer 2016; Jin, Zhang, and Li 2018; Lee et al. 2020)
achieve high performance on CIFAR-10 and other neuroï¿¾morphic datasets. Among them, (Wu et al. 2019) improves
the leaky integrate-and-fire (LIF) model (Hunsberger and
Eliasmith 2015) to an iterative LIF model and develop
STBP learning algorithm, which makes it friendly on ANNï¿¾oriented programming frameworks and speeds up the trainï¿¾ing process. Moreover, training SNNs directly shows great
potential on dealing with spatial and temporal information
and reports high accuracy within very few timesteps. Howï¿¾ever, it fails to train a very deep SNN directly because of
the gradient vanishing and internal covariate shift, which is
exactly what we want to conquer.
Gradient vanishing or explosion in the deep neural netï¿¾work (DNN) A DNN can avoid gradient vanishing or exï¿¾plosion when it is dynamic isometry, which means every
singular value of its input-output jacobian matrix remains
close to 1. (Chen et al. 2020) proposes a metricâ€”â€œBlock
Dynamical Isometryâ€, serving as a general statistical tool
to all of complex serial-parallel DNN. It investigates the first
and second moment of each block in the neural network and
analyze their effects to the gradient distribution. Moreover,
it gives theoretical explanation to the function of the weight
initialization, batch normalization and shortcut connection
in the neural network, which helps us to develop our algoï¿¾rithm.
Normalization Normalization techniques enable the
training of well-behaved neural networks. For artificial
neural networks, normalization, such as batch normalization
(Ioffe and Szegedy 2015), group normalization (Wu and
He 2018), and layer normalization (Ba, Kiros, and Hinton
2016), have become common methods. Batch normalization
(BN) accelerates deep networks training by reducing
internal covariate shift, which enables higher learning rates
and regularizes the model. While it causes high learning
latency and increases computation, BN makes it possible for
networks to go deeper avoiding gradient vanishing or exï¿¾plosion. For SNNs, researchers propose other normalization
techniques, such as data-based normalization (Diehl et al.
2015), Spike-Norm (Sengupta et al. 2019) and NeuNorm
(Wu et al. 2019). These normalization methods aim to
balance the input and threshold to avoid serious information
loss, but they are not effective to our directly-trained deep
SNNs because they still neglect the problem of gradient
vanishing. We noticed the effects of BN in ANNs and the
importance of input distribution in SNNs, so we modify BN
to satisfy the training and inference of SNN models.
Materials and Methods
Iterative LIF model
The iterative LIF model is first proposed by (Wu et al. 2019),
who utilizes Euler method to solve the first-order differential
equation of Leaky integrate-and-fire (LIF) model and conï¿¾verts it to an iterative expression
u
t = Ï„decayu
tâˆ’1 + I
t
, (1)
where Ï„decay is a constant to describe how fast the memï¿¾brane potential decays, u
t
is the membrane potential, I
t
is
the pre-synaptic inputs. Let Vth denote the given threshold.
When u
t > Vth, the neuron fires a spike and u
t will be reset
to 0. The pre-synaptic inputs are accumulated spikes from
other neurons at the last layer. So I
t
can be represented by
x
t =
P j wjo
t
(j), where wj are weights and o
t
(j) denotes
binary spiking output from others at the moment of t. Takï¿¾ing spatial structure into consideration and set ureset = 0,
the whole iterative LIF model in both spatial and temporal
domain can be determined by
u
t,n+1 = Ï„decayu
tâˆ’1,n+1(1 âˆ’ o
tâˆ’1,n+1) + x
t,n
, (2)
o
t,n+1 =

1 if u
t,n+1 > Vth,
0 otherwise.
(3)
where u
t,n is the membrane potential of the neuron in nï¿¾th layer at time t, o
t,n is the binary spike and Ï„decay is the
potential decay constant.
The iterative LIF model enables forward and backward
propagation to be implemented on both spatial and tempoï¿¾ral dimensions, which makes it friendly to general machineï¿¾learning programming frameworks.
Threshold-dependent batch normalization
As a regular component of DNNs, batch normalization (BN)
has been a common method for current neural networks,
which allows stable convergence and much deeper neural
networks. However, because of the additional temporal diï¿¾mension and special activation mechanism, directly-trained
SNNs need a specially-designed normalization method.
That motivates us to propose the threshold-dependent batch
normalization (tdBN).
We consider a Spiking Convolution Neural Network
(SCNN). Let o
t
represent spiking outputs of all neurons in a
layer at timestep t. With convolution kernel W and bias B,
we have
x
t = W ~ o
t + B, (4)
where x
t âˆˆ RNÃ—CÃ—HÃ—W represents the pre-synapse inputs
at timestep t with N as the batch axis, C as the channel axis,
(H, W) as the spatial axis.
In our tdBN, the high-dimensional pre-synaptic inputs
will be normalized along the channel dimension (Fig. 1).
Let x
t
k
represent k-th channel feature maps of x
t
. Then
xk = (x
1
k
, x2
k
, Â· Â· Â· , xT
k
) will be normalized by
xË†k =
Î±Vth(xk âˆ’ E[xk])
p
V ar[xk] + 
, (5)
yk = Î»kxË†k + Î²k, (6)
where Vth denotes the threshold, Î± is a hyper-parameter deï¿¾pending on network structure,  is a tiny constant, Î»k and Î²k
are two trainable parameters, E[xk], V ar[xk] are the mean
and variance of xk statistically estimated over the Miniï¿¾Batch. Fig. 1 displays how E[xk], V ar[xk] to be computed,
which are defined by
E[xk] = mean(xk), (7)
V ar[xk] = mean((xk âˆ’ E[xk])2
). (8)
So, during the training, yk âˆˆ RT Ã—NÃ—HÃ—W is exactly the
normalized pre-synaptic inputs received by neurons of k-th
channel at the next layer during T timesteps.
N C
ğ‘¥
2
N C
ğ‘¥
1
N C
ğ‘¥
ğ‘‡
E [ğ‘¥1]
N
T
Feature maps
Mean
Square; Mean
C-dimensional vector
Tensor for estimation
ğ‘¥k
ğ‘¥
E [ğ‘¥2]
E [ğ‘¥k]
E [ğ‘¥C]
Var [ğ‘¥1]
C-dimensional vector
Var [ğ‘¥2]
Var [ğ‘¥k]
Var [ğ‘¥C]
k-th channel
Figure 1: Estimation of E[x] and V ar[x] in tdBN. Each
cube shows a feature map tensor at t timestep, with N as
the batch axis, C as the channel axis, (H, W) as the spatial
axis. Each element in C-dimension vector E[x] and V ar[x]
is estimated by the yellow tensor of corresponding channel.
In the inference, we follow the schema as standard batch
normalization to estimate Âµinf and Ïƒinf
2
that represent the
expectation of E[xk] and V ar[xk] respectively over the
whole datasets, which can be computed during the training
process by moving average solution.
Moreover, the batchnorm-scale-fusion is necessary to
SNNs with tdBN in inference. It removes the batch normalï¿¾ization operations during the inference, thereby maintaining
the network to be full-spiking and enabling it to be impleï¿¾mented on the neuromorphic platforms. Let Wc,k and Bc,k
denote the convolution kernel and bias between the c-th feaï¿¾ture map in a layer and the k-th feature map in the next layer.
The schema is determined by
Wc,k
0 = Î»k
Î±VthWc,k
q
Ïƒinf,k
2 + 
, (9)
Bc,k
0 = Î»k
Î±Vth(Bc,k âˆ’ Âµinf,k)
q
Ïƒinf,k
2 + 
+ Î²k, (10)
where Wc,k
0 and bias Bc,k
0 denote the transformed weights
after the batchnorm-scale-fusion. Thus, during the inference,
spikes propagate layer by layer through transformed weights
Wc,k
0 and bias Bc,k
0 without batchnorm operations. Thereï¿¾fore, our tdBN only affects the computation costs during
H,W H,W H,W
H,W Â·Â·Â· Â·Â·Â· Â·Â·Â· Â·Â·Â· Â·Â·Â·
Â·Â·Â· Â·Â·Â· Â·Â·Â· Â·Â·Â·
training and doesnâ€™t influence the running mechanism of
SNNs already trained.
In short, our tdBN has two main differences from the stanï¿¾dard BN. Firstly, unlike ANNs, SNNs propagate information
not only layer by layer but also from last moment to the next.
So, tdBN should normalize feature inputs on both temporal
and spatial dimensions. Secondly, we make normalized variï¿¾ance dependent on threshold. In tdBN, pre-activations are
normalized to N(0,(Î±Vth)
2
) instead of N(0, 1). And we
will initialize the trainable parameters Î» and Î² with 1 and
0. In the serial neural network, the hyper-parameter Î± is 1.
For a local parallel network structure having n branches, Î±
will be 1/
âˆš
n. It makes the pre-activations with mean of 0
and standard deviation of Vth at the early training. The codes
of the tdBN can be found in Supplementary Material A.
Overall training algorithm
In this section, we present the overall training algorithm of
the STBP-tdBN for training deep SNNs from scratch with
our tdBN.
In the error backpropagation, we consider the last layer as
the decoding layer, and the final output Q can be determined
by
Q =
T
1
T
X
t=1
Mon,t
, (11)
where o
n,t is the spike fired by the last output layer, M is
the matrix of decoding layer and T denotes the number of
timesteps.
Then we make the outputs pass through a softmax layer.
The loss function is determined as the cross-entropy. Conï¿¾sidering the output Q = (q1, q2, Â· Â· Â· , qn) and label vector
Y = (y1, y2, Â· Â· Â· , yn), loss function L is defined by
pi =
e
qi
P
n
j=1 e
qj
, (12)
L = âˆ’
nX
i=1
yi
log(pi). (13)
With the iterative LIF model, STBP-tdBN method backï¿¾propagates the gradient of the loss L on both spatial and temï¿¾poral domains. By applying the chain rule, âˆ‚o
âˆ‚L
t,n
i
and âˆ‚u
âˆ‚L
t,n
i
can be computed by
âˆ‚L
âˆ‚ot,n
i
=
l(n+1)
X
j=1
âˆ‚L
âˆ‚ut,n+1
j
âˆ‚ut,n+1
j
âˆ‚ot,n
i
+
âˆ‚L
âˆ‚ut+1,n
i
âˆ‚ut+1,n
i
âˆ‚ot,n
i
,
(14)
âˆ‚L
âˆ‚ut,n
i
=
âˆ‚L
âˆ‚ot,n
i
âˆ‚ot,n
i
âˆ‚ut,n
i
+
âˆ‚L
âˆ‚ut
i
+1,n
âˆ‚ut
i
+1,n
âˆ‚ut,n
i
, (15)
where o
t,n and u
t,n represent the spike and membrane poï¿¾tential of the neuron in layer n at time t. Because of the nonï¿¾differentiable spiking activities, âˆ‚ot
âˆ‚ut doesnâ€™t exist in fact.
To solve this problem, (Wu et al. 2018) proposes derivative
curve to approximate the derivative of spiking activity. In
this paper, we use the rectangular function, which is proved
to be efficient in gradient descent and can be determined by
âˆ‚ot
âˆ‚ut
=
1
a
sign(|u
t âˆ’ Vth| <
a
2
). (16)
The codes of the overall training algorithm are shown in
Supplementary Material A.
Theoretical Analysis
In this section, we will analyze the effects of tdBN to SNNs
trained by STBP-tdBN. With theoretical tools about gradient
norm theory in ANNs, we find that our tdBN can alleviate
the problem of gradient vanishing or explosion during the
training process. We also explain the functions of scaling
factors Î± and Vth we added during the normalization.
Gradient norm theory
The gradient norm theory has been developed well in reï¿¾cent years, which aims to conquer the gradient vanishing or
explosion in various neural networks structure. In this paï¿¾per, we adopt the â€œBlock Dynamical Isometryâ€ proposed
by (Chen et al. 2020) to analyze tdBNâ€™s effect in directlyï¿¾trained SNNs. It considers a network as a series of blocks
f(x) = fi,Î¸i
â—¦ fiâˆ’1,Î¸iâˆ’1 â—¦ Â· Â· Â· â—¦ f1,Î¸1
(x), (17)
where the function fj,Î¸j
represents the j
th block and define
its input-output jacobian matrix as âˆ‚f
âˆ‚fj
jâˆ’1
= Jj . Let Ï†(J)
represent the expectation of tr(J) and Ï•(J) denote Ï†(J
2
)âˆ’
Ï†
2
(J). Then they proved the following lemmas.
Lemma 1. Consider a neural network that can be repreï¿¾sented as a series of blocks as Eq. (17) and the j
th blockâ€™s
jacobian matrix is denoted as Jj . If âˆ€j, Ï†(JjJj
T
) â‰ˆ 1 and
Ï•(JjJj
T
) â‰ˆ 0 , the network achieves â€œBlock Dynamical
Isometryâ€ and can avoid gradient vanishing or explosion.
(Chen et al. 2020)
Lemma 2. Consider a block of neural network, which conï¿¾sists of data normalization with 0-mean, linear transform
and rectifier activations (â€œ General Linear Transformâ€).
Let 2
nd moment of input vector as Î±in and the output vector
as Î±out, we have Ï†(JJT
) = Î±
Î±
out
in
. (Chen et al. 2020)
Based on the theoretical framework of gradient norm, we
combine it with the unique properties of spiking neurons and
further analyze the effectiveness of our proposed tdBN algoï¿¾rithm for SNNs.
LIF model has two special hyper-parameters: Ï„decay and
Vth, where Ï„decay influences the gradients propagation in
temporal domain and Vth effects the spatial dimension. For
experiment with SNNs, the Ï„decay are often set as small
value (e.g. 0.25). To analyze the gradient transformation, we
simplify the model and set Ï„decay as zero and we can get the
following proposition.
Theorem 1. Consider an SNN with T timesteps and the j
th
blockâ€™s jacobian matrix at time t is denoted as Jj
t
. When
Ï„decay is equal to 0, if we fix the second moment of input
vector and the output vector to Vth
2
for each block between
two tdBN layers, we have Ï†(Jj
t
(Jj
t
)
T
) â‰ˆ 1 and the training
of SNN can avoid gradient vanishing or explosion.
Proof. The proof of Theorem 1 is based on the Lemma 1
and Lemma 2. The details are presented in Supplementary
Material B.
Influence of membrane decay mechanism We analyze
the effect of Ï„decay to the gradient propagation. From equaï¿¾tions (2) and (15), we have
âˆ‚L
âˆ‚ut,n
i
=
âˆ‚L
âˆ‚ot,n
i
âˆ‚ot,n
i
âˆ‚ut,n
i
+
âˆ‚L
âˆ‚ut
i
+1,n
Ï„decay(1 âˆ’ o
t,n
i
). (18)
That is to say, if a neuron fires a spike, (1 âˆ’ o
t,n
i
) is equal
to zero and the gradient is irrelevant to Ï„decay. On the other
hand, for Ï„decay is a tiny constant, the gradient of the neuron
at time t + 1 has little influence to that at time t.
To verify the Theorem 1 and our analysis about influence
of membrane decay mechanism, we evaluate our tdBN in
20-layers plain spiking network on CIFAR-10. In Fig. 2 ,
we display the mean of gradient norms in each layer during
the first 1/6 epoch of training process despite the first encodï¿¾ing layer and the last decoding layer. And we find that when
Ï„decay = 0, the curve of gradient norm behaves much steady,
which perfectly supports our theory. It should be emphasized
that Ï„decay cannot be set as 0 because it will prevent the inï¿¾formation from propagating along temporal dimension and
cause serious degradation. So, we evaluate our method in the
condition that Ï„decay = 0. For example, when Ï„decay = 0.25
and 0.5, the gradient norm increases very slowly as the netï¿¾work deepens, which will not influence the training process.
The results strongly support our results that can avoid gradiï¿¾ent vanishing or explosion in deep SNNs.
0 4 8 12 16
Layer
10
5
10
4
decay = 0
decay = 0.25
decay = 0.5
Figure 2: Gradient norm throughout the plain network
with tdBN.
Scaling factors
As we all know, a key for SNN model to obtain competitive
performance is to set suitable threshold to maintain firing
rates and reduce information loss. To achieve this, we inï¿¾troduce two scaling factors to the normalization implements
in tdBN, which is meant to balance the pre-activations and
threshold. In the early training, with two scaling factorsâ€”Î±
and Vth, we normalize the pre-activations to N(0, Vth
2
) by
initializing the trainable parameters Î» and Î² with 1 and 0.
First, we propose Theorem 2 to explain the relations beï¿¾tween per-activations and membrane potential of neurons,
which helps to understand why our method works.
Theorem 2. With the iterative LIF model, assuming the preï¿¾activations x
t âˆ¼ N(0, Ïƒin
2
), we have the membrane potenï¿¾tial u
t âˆ¼ N(0, Ïƒout
2
) and Ïƒout
2 âˆ Ïƒin
2
.
Proof. The proof of Theorem 2 is presented in Supplemenï¿¾tary Material B.
2 0 2
Value
0.0
0.2
0.4
0.6
0.8
1.0
(a)
x
t N(0, 1/4), Vth = 1
u
t
2 0 2
Value
0.0
0.2
0.4
0.6
0.8
1.0
(b)
x
t N(0, 1/2), Vth = 1
u
t
0 5
Value
0.0
0.2
0.4
0.6
0.8
1.0
(c)
x
t N(0, 1), Vth = 1
u
t
Figure 3: Distributions of membrane potential u
t with
different variances of pre-activations x
t
We verify the Theorem 2 by visualized analysis. In the
experiment, we set Ï„decay = 0.25 and display the distriï¿¾bution of membrane potential with different pre-activations
variance Ïƒin
2
. The results are shown in Fig. 3. We find
the high degree of similarity between the distributions of
pre-activations and membrane potential, which supports the
proposition.
Next, we analyze the forward information propagation
mechanism with the LIF model.
0.00 0.25 0.50 0.75 1.00
Firing rates
0
2000
4000
6000
8000
10000
(a)
x
t N(0, 1/16), Vth = 1
0.00 0.25 0.50 0.75 1.00
Firing rates
0
1000
2000
3000
4000
5000
(b)
x
t N(0, 1), Vth = 1
0.00 0.25 0.50 0.75 1.00
Firing rates
0
500
1000
1500
2000
(c)
x
t N(0, 16), Vth = 1
Figure 4: Distributions of neuronsâ€™ firing rates with difï¿¾ferent variances of pre-activations x
t
.
During the forward, when membrane potential reaches the
threshold, neurons will fire a spike and make the informaï¿¾tion propagate layer by layer. With Theorem 2 and Eq. (3)
, we can approximate the possibilities of neurons firing a
spike P(u
t > Vth). Itâ€™s obvious that it is a positive correï¿¾lation between P(u
t > Vth) and the variance of membrane
|| || /
2
Frequency
Numbers of nuerons
potential Ïƒout
2
as well as Ïƒin
2
. So, we adopt the scaling facï¿¾tors to adjust the distributions of pre-activations to maintain
the firing rates in deep SNNs. Fig. 4 shows the distribuï¿¾tion of neuronsâ€™ firing rates when we set variances of preï¿¾activations x
t âˆ¼ N(0, Ïƒin
2
) as different values. Because of
the decay mechanisms, even a neuron receives positive input
each time, it may fire no spike (Fig. 4(a)), which means neuï¿¾rons in next layer only receive few nonzero pre-synaptic inï¿¾puts, making spikes disappear in deep SNNs and preventing
signal from propagating forward. Another situation is that a
neuron fires spikes all the time (Fig. 4(c)), which means the
outputs of some neurons to be insensitive to the change of
pre-activations and causes increase of computation.
In conclusion, to balance pre-synaptic input and threshï¿¾old to maintain the firing rates, we utilize the scaling facï¿¾tors to control the variance of membrane potential and preï¿¾activations, which alleviates its dependence on the threshold.
So, we normalize the pre-activations to N(0, Vth
2
).
Deep Spiking Residual Network
ResNet is one of the most popular architectures o tackle with
the problem of degradation when networks go deep. With
the shortcut connections, (He et al. 2016) adds identity mapï¿¾ping between layers, which enables the training of very deep
neural networks. Inspired by the residual learning, we proï¿¾pose our deep spiking residual network, which replaces the
BN layer with our tdBN and changes the shortcut connection
to achieve better performance.
Basic block ResNet in ANNs is built with some basic
blocks. Fig. 5(a) shows a form of traditional basic blocks in
ResNet-ANN. It is implemented in relatively shallow residï¿¾ual networks, which consists of two layers with 3 Ã— 3 conï¿¾volution kernel followed by a BN layer and the ReLU acï¿¾tivation. On the basis, we propose the basic block of our
deep spiking residual network. As shown in Fig. 5(b), we
replace the ReLU activation with LIF model and replace the
BN layer with our tdBN. Moreover, we change the shortcut
connection and add a tdBN layer before the final addition.
Hence, expect that the hyper-parameters Î± in tdBN layers
before the final activation layer or in the shortcut are set as
1/
âˆš
2, the other tdBN layersâ€™ hyper-parameter Î± is defined
as 1. It guarantees the input distribution of each activation
will satisfy N(0, Vth
2
) at the beginning of training.
Network architecture We build the deep spiking Residual
network with our basic blocks. The first encoding layer reï¿¾ceives the inputs and performs downsampling directly with
convolution kernel and a stride of 2. Then, the spiking acï¿¾tivities propagate through basic blocks. Similar to ResNetï¿¾ANN, we double the number of channels when the feature
map is halved. After average pooling or full-connected layer
if necessary, the last decoding layer is a fully-connected
layer followed by softmax function.
Experiment
We test our deep residual SNNs on static non-spiking
datasets (CIFAR-10, ImageNet) and neuromorphic datasets
3Ã—3 Conv
BN
ReLU
3Ã—3 Conv
ReLU
BN
+
Real-value inputs
Real-value outputs
3Ã—3 Conv
tdBN,ğ›¼ = 1
LIF
3Ã—3 Conv
LIF
tdBN,ğ›¼ = 1/ 2
+
Spiking inputs
Spiking outputs
(a) (b) Spike trains
tdBN,ğ›¼ = 1/ 2
Conversion
Add
Conversion
Figure 5: Different basic blocks between traditional
ResNet-ANN (a) and our ResNet-SNN with tdBN (b).
(DVS-gesture, DVS-CIFAR10). And we compare our reï¿¾sults with SNN models to demonstrate the advantages of
our method on both accuracy and the number of timesteps.
The dataset introduction, pre-processing, parameter configï¿¾uration, training details and results analysis are summarized
in Supplementary Material C.
Experiment on static datasets
Experiments on static datasets include CIFAR-10 and Imï¿¾ageNet, which serve as standard image recognition benchï¿¾marks. We test our ResNet-SNN with different timesteps,
sizes and depths. With very few timesteps, our models sigï¿¾nificantly reduce the amount of computation compared with
ANNs with same architectures due to sparse spikes. The deï¿¾tailed analysis is presented in Supplementary Material D.
CIFAR-10 CIFAR-10 is an image dataset with 50000
training images and 10000 testing images with size of 32 Ã—
32, which all belong to 10 classes. In this experiment, we
use ResNet-19 with timesteps of 2, 4, 6.
Our results are shown in the Table 1. Before us, SNNs usï¿¾ing the ANN-SNN converted method report best accuracy
on CIFAR-10. However, to achieve good performance, conï¿¾verted SNNs usually require more than 100 timesteps. In
this work, our model achieves state-of-the-art performance
(93.15% top-1 accuracy with only 6 timesteps) on CIFAR-
10, which not only greatly reduces latency and computation
cost compared with other SNN models.
ImageNet ImageNet (Deng et al. 2009) contains 1.28 milï¿¾lion training images and 50000 validating images. On Imaï¿¾geNet, we test ResNet-34 with standard size and large size.
The large model doubles the channels compared with the
standard one and achieves 67.05% top-1 accuracy with
just 6 timesteps. Also, we use ResNet-50 to explore the
very deep directly-trained SNNs and achieve 64.88% top-1
accuracy. Similar to ResNet-ANN, we observe the enhanceï¿¾ment of accuracy between ResNet-34 and ResNet-50, which
Dataset Model Methods Architecture TimeStep Accuracy
CIFAR-10
(Sengupta et al. 2019) ANN-SNN VGG-16 2500 91.55%
(Hu et al. 2018) ANN-SNN ResNet-44 350 92.37%
(Rathi et al. 2020) Hybird Training VGG-16 200 92.02%
(Lee et al. 2020) Spike-based BP ResNet-11 100 90.95%
(Wu et al. 2019) STBP 5 Conv, 2Fc 12 90.53%
our model STBP-tdBN ResNet-19
6 93.16%
4 92.92%
2 92.34%
ImageNet
(Sengupta et al. 2019) ANN-SNN VGG-16 2500 69.96%
(Sengupta et al. 2019) ANN-SNN ResNet-34 2500 65.47%
(Han, Srinivasan, and Roy 2020) ANN-SNN ResNet-34 1024 66.61%
(Hu et al. 2018) ANN-SNN ResNet-34 768 71.61%
(Rathi et al. 2020) Hybird Training VGG-16 250 65.19%
(Rathi et al. 2020) Hybird Training ResNet-34 250 61.48%
our model STBP-tdBN
ResNet-34 6 63.72%
ResNet-50 6 64.88%
ResNet-34(large) 6 67.05%
Table 1: Comparisons with SNNs on CIFAR-10 & ImageNet
shows the deeper residual spiking networks may perform
better in complex recognition tasks.
Before us, the directly-trained full-spiking SNNs have
never reported competitive results on ImageNet while our
methods make a breakthrough. Compared with early works,
our methods achieve higher accuracies with fewer timesteps.
The results are shown in Table 1.
Experiment on neuromorphic datasets
Compared with non-spiking static datasets, neuromorphic
datasets contain more temporal information, which are more
suitable for SNNs to demonstrate their advantages. Previous
work (Wu et al. 2019) only tests the MLP-SNN with STBP
on N-MNIST and report the state-of-art results (98.57% acï¿¾curacy). However, N-MNIST is too simple for our deep
directly-trained SNNs with residual structures. So, we adopt
two challenging neuromorphic datasetsâ€”DVS-Gesture and
DVS-CIFAR10.
DVS-Gesture DVS-Gesture (Amir et al. 2017) is a collecï¿¾tion of moving gestures performed by 29 different individuï¿¾als belonging to 11 classes, which is captured by DVS camï¿¾eras under three lighting conditions. We set timestep T to
be 40. In each timestep, the network receives only one slice
of the event stream, which means the first 1200ms of each
action is used for training or testing. In this experiment, we
use ResNet-17 and achieve an accuracy of 96.87%, which is
the state-of-the-art result for directly-trained SNNs on DVSï¿¾Gesture. We compare our results with other related works on
DVS-Gesture, as shown in Table 2.
Model Methods Accuracy
(He et al. 2020) STBP 93.40%
(Shrestha and Orchard 2018) SLAYER 93.64%
(Kugele et al. 2020) ANN 95.68%
(Amir et al. 2017) BPTT 94.59%
our model STBP-tdBN 96.87%
Table 2: Accuracy comparisons on DVS-Gesture
DVS-CIFAR10 DVS-CIFAR10 (Li et al. 2017) is a neuï¿¾romorphic dataset converted from famous CIFAR-10 to its
dynamic form. It consists of 1000 images in the format of
spike train per class. With the noisy environment, itâ€™s also a
challenging recognition task similar to DVS-Gesture. With
ResNet-19, our methods achieve the best performance with
67.8% accuracy in 10 timesteps. Table 3 compares our reï¿¾sults with other models.
Model Methods Accuracy
(Sironi et al. 2018) HATS 52.40%
(Kugele et al. 2020) Streaming rollout ANN 66.75%
(Ramesh et al. 2019) DART 65.78%
(Wu et al. 2019) STBP 60.5%
our model STBP-tdBN 67.8%
Table 3: Accuracy comparisons on DVS-CIFAR10
Conclusion
In this paper, we present a normalization method enabling
directly-trained deep SNNs with high performance. We
combine the gradient norm theory and prove that this
method can effectively balance the input stimulus and
neuronal threshold during training, thereby facilitating the
learning convergence. On this basis, by further introducing
the shortcut connection, we greatly extend directly-trained
SNNs from a common shallow structure (less than ten lay-
ers) to a very deep structure (more than fifty layers). Fiï¿¾nally, the model is evaluated on both large-scale static image
datasets and neuromorphic datasets. Comparing with other
SNN models, we achieve a high accuracy on CIFAR-10 and
ImageNet with a significantly small inference latency. To
our best knowledge, it is the first work to report a directlyï¿¾trained and very deep SNNs on ImageNet. On neuromorphic
datasets, our model can efficiently process temporal-spatial
information and achieve state-of-the-art performance.
In summary, this work provides a feasible directly-trained
scheme for deep SNNs. It maintains high efficiency of
spike-based communication mechanism and enables SNNs
to solve more complex large-scale classification tasks, which
may benefit the implementations on the neuromorphic hardï¿¾ware and promote the practical applications of SNNs.
Acknowledgment
This work is partially supported by National Key R&D Proï¿¾gram of China (No.2018YFE0200200,2018AAA0102600),
Beijing Academy of Artificial Intelligence (BAAI), and a
grant from the Institute for Guo Qiang of Tsinghua uniï¿¾versity, and in part by the Science and Technology Major
Project of Guangzhou (202007030006), and the key scienï¿¾tific technological innovation research project by Ministry
of Education, and the open project of Zhejiang laboratory.
References
Amir, A.; Taba, B.; Berg, D.; Melano, T.; McKinstry, J.;
Di Nolfo, C.; Nayak, T.; Andreopoulos, A.; Garreau, G.;
Mendoza, M.; Kusnitz, J.; Debole, M.; Esser, S.; Delbruck,
T.; Flickner, M.; and Modha, D. 2017. A Low Power, Fully
Event-Based Gesture Recognition System. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR).
Ba, J. L.; Kiros, J. R.; and Hinton, G. E. 2016. Layer Norï¿¾malization. arXiv preprint arXiv:1607.06450 .
Chen, Z.; Deng, L.; Wang, B.; Li, G.; and Xie, Y. 2020. A
Comprehensive and Modularized Statistical Framework for
Gradient Norm Equality in Deep Neural Networks. IEEE
Transactions on Pattern Analysis and Machine Intelligence
1â€“1.
Davies, M.; Srinivasa, N.; Lin, T.; Chinya, G.; Cao, Y.; Choï¿¾day, S. H.; Dimou, G.; Joshi, P.; Imam, N.; Jain, S.; Liao, Y.;
Lin, C.; Lines, A.; Liu, R.; Mathaikutty, D.; McCoy, S.; Paul,
A.; Tse, J.; Venkataramanan, G.; Weng, Y.; Wild, A.; Yang,
Y.; and Wang, H. 2018. Loihi: A Neuromorphic Manycore
Processor with On-Chip Learning. IEEE Micro 38(1): 82â€“
99.
Deng, J.; Dong, W.; Socher, R.; Li, L.; Kai Li; and Li
Fei-Fei. 2009. ImageNet: A large-scale hierarchical image
database. In 2009 IEEE Conference on Computer Vision and
Pattern Recognition, 248â€“255.
Diehl, P. U.; Neil, D.; Binas, J.; Cook, M.; Liu, S.; and
Pfeiffer, M. 2015. Fast-classifying, high-accuracy spiking
deep networks through weight and threshold balancing. In
2015 International Joint Conference on Neural Networks
(IJCNN), 1â€“8.
Han, B.; Srinivasan, G.; and Roy, K. 2020. RMP-SNN:
Residual Membrane Potential Neuron for Enabling Deeper
High-Accuracy and Low-Latency Spiking Neural Network.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR).
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep Residï¿¾ual Learning for Image Recognition. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogniï¿¾tion (CVPR).
He, W.; Wu, Y.; Deng, L.; Li, G.; Wang, H.; Tian, Y.; Ding,
W.; Wang, W.; and Xie, Y. 2020. Comparing SNNs and
RNNs on Neuromorphic Vision Datasets: Similarities and
Differences. arXiv preprint arXiv:2005.02183 .
Hu, Y.; Tang, H.; Wang, Y.; and Pan, G. 2018. Spiking Deep
Residual Network. arXiv preprint arXiv:1805.01352 .
Hunsberger, E.; and Eliasmith, C. 2015. Spiking Deep Netï¿¾works with LIF Neurons. arXiv preprint arXiv:1510.08829
.
Ioffe, S.; and Szegedy, C. 2015. Batch Normalization: Acï¿¾celerating Deep Network Training by Reducing Internal Coï¿¾variate Shift. arXiv preprint arXiv:1502.03167 .
Jin, Y.; Zhang, W.; and Li, P. 2018. Hybrid Macro/Micro
Level Backpropagation for Training Deep Spiking Neural
Networks. In Advances in Neural Information Processing
Systems 31, 7005â€“7015.
Kugele, A.; Pfeil, T.; Pfeiffer, M.; and Chicca, E. 2020. Ef-
ficient Processing of Spatio-Temporal Data Streams With
Spiking Neural Networks. Frontiers in Neuroscience 14:
439.
Lee, C.; Sarwar, S. S.; Panda, P.; Srinivasan, G.; and Roy, K.
2020. Enabling Spike-Based Backpropagation for Training
Deep Neural Network Architectures. Frontiers in Neuroï¿¾science 14: 119.
Lee, J. H.; Delbruck, T.; and Pfeiffer, M. 2016. Trainï¿¾ing Deep Spiking Neural Networks Using Backpropagation.
Frontiers in Neuroscience 10: 508.
Li, H.; Liu, H.; Ji, X.; Li, G.; and Shi, L. 2017. CIFAR10-
DVS: An Event-Stream Dataset for Object Classification.
Frontiers in Neuroscience 11: 309.
Merolla, P. A.; Arthur, J. V.; Alvarez-Icaza, R.; Cassidy,
A. S.; Sawada, J.; Akopyan, F.; Jackson, B. L.; Imam, N.;
Guo, C.; Nakamura, Y.; Brezzo, B.; Vo, I.; Esser, S. K.; Apï¿¾puswamy, R.; Taba, B.; Amir, A.; Flickner, M. D.; Risk,
W. P.; Manohar, R.; and Modha, D. S. 2014. A million
spiking-neuron integrated circuit with a scalable communiï¿¾cation network and interface. Science 345(6197): 668â€“673.
Ramesh, B.; Yang, H.; Orchard, G. M.; Le Thi, N. A.;
Zhang, S.; and Xiang, C. 2019. DART: Distribution Aware
Retinal Transform for Event-based Cameras. IEEE Transï¿¾actions on Pattern Analysis and Machine Intelligence 1â€“1.
Rathi, N.; Srinivasan, G.; Panda, P.; and Roy, K. 2020. Enï¿¾abling Deep Spiking Neural Networks with Hybrid Converï¿¾sion and Spike Timing Dependent Backpropagation. arXiv
preprint arXiv:2005.01807 .
Roy, K.; Jaiswal, A.; and Panda, P. 2019. Towards spikeï¿¾based machine intelligence with neuromorphic computing.
Nature 575(7784): 607-617.
Sengupta, A.; Ye, Y.; Wang, R.; Liu, C.; and Roy, K. 2019.
Going Deeper in Spiking Neural Networks: VGG and Residï¿¾ual Architectures. Frontiers in Neuroscience 13: 95.
Shrestha, S. B.; and Orchard, G. 2018. SLAYER: Spike
Layer Error Reassignment in Time. In Bengio, S.; Wallach,
H.; Larochelle, H.; Grauman, K.; Cesa-Bianchi, N.; and Garï¿¾nett, R., eds., Advances in Neural Information Processing
Systems 31, 1412â€“1421.
Sironi, A.; Brambilla, M.; Bourdis, N.; Lagorce, X.; and
Benosman, R. 2018. HATS: Histograms of Averaged Time
Surfaces for Robust Event-Based Object Classification. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 1731â€“1740.
Stromatias, E.; Neil, D.; Pfeiffer, M.; Galluppi, F.; Furber,
S. B.; and Liu, S.-C. 2015. Robustness of spiking Deep Beï¿¾lief Networks to noise and reduced bit precision of neuroï¿¾inspired hardware platforms. Frontiers in Neuroscience 9:
222.
Wu, Y.; Deng, L.; Li, G.; Zhu, J.; and Shi, L. 2018. Spatioï¿¾Temporal Backpropagation for Training High-Performance
Spiking Neural Networks. Frontiers in Neuroscience 12:
331.
Wu, Y.; Deng, L.; Li, G.; Zhu, J.; Xie, Y.; and Shi, L. 2019.
Direct training for spiking neural networks: Faster, larger,
better. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 33, 1311â€“1318.
Wu, Y.; and He, K. 2018. Group normalization. In Proï¿¾ceedings of the European Conference on Computer Vision
(ECCV), 3â€“19.
Supplementary Material
A Codes for Algorithms
In this section, we present the detailed codes for (1)
threshold-dependent batch normalization (algorithm 1) (2)
overall training algorithm (algorithm 2).
Algorithm 1 Threshold-dependent BatchNorm
In the Training:
Input: weighted outputs from last layer
x âˆˆ [T imestep, Batch size, Cout, H, W].
Output: normalized pre-synaptic inputs to the next layer
y âˆˆ [T imestep, Batch size, Cout, H, W].
Parameters: two trainable parameters Î», Î² âˆˆ [Cout],
threshold Vth, hyper-parameter Î±.
Initialize:Î» â† 1, Î² â† 0.
1: Âµtra = mean(x[cout :])
2: Ïƒtra
2 = mean(square(x[cout :] âˆ’ Âµtra))
3: xË† =
Î±Vth(xâˆ’Âµtra)
âˆš
Ïƒ
2
tra+
4: y = Î»xË† + Î²
In the Inference:
Input: convolution kernel W âˆˆ [Cout, Cin, H, W], bias
B âˆˆ [Cout], spiking outputs from last layer o âˆˆ
[T imestep, Batch size, Cin, H, W].
Output: pre-synaptic inputs to the next layer y
0 âˆˆ
[T imestep, Batch size, Cout, H, W].
Parameters: two trainable parameters from tdBN for trainï¿¾ing Î», Î² âˆˆ [Cout], threshold Vth, hyper-parameter Î±.
1: Âµinf â† E(Âµtra)
2: Ïƒinf
2 â† E(Ïƒtra
2
)
3: W0 = Î»
Î±VthW
âˆšÏƒinf
2 +
4: B0 = Î»
Î±Vth(Bâˆ’Âµinf )
âˆšÏƒ
2
inf +
+ Î²
5: for t = 1 to T do
6: y
0 [t] = W0 ~ o[t] + B0
7: end for
B Proofs of Theorems
Theorem 1. Consider an SNN with T timesteps and the j
th
blockâ€™s jacobian matrix at time t is denoted as Jj
t
. When
Ï„decay is equal to 0, if we fix the second moment of input
vector and the output vector to Vth
2
for each block between
two tdBN layers, we have Ï†(Jj
t
(Jj
t
)
T
) â‰ˆ 1 and the training
of SNN can avoid gradient vanishing or explosion.
Proof. Consider an SNN with T time step. When Ï„decay is
equal to 0, the membrane potential only depends on the inï¿¾put signal,so the gradient transforms independently in each
timestep.
At the timestep t, we consider the SNN as a series of
blocks between two tdBN layers
f
t
(x) = fi,Î¸
t
i
â—¦ fi
t
âˆ’1,Î¸iâˆ’1 â—¦ Â· Â· Â· â—¦ f1
t
,Î¸1
(x), (19)
Algorithm 2 Overall Training Algorithm
Input: input X, label vector Y .
Output: parameters in layer i Wi
,Bi
, prediction result Q.
1: function NeuronU pdate(u, I)
2: for t = 1 to T do
3: u[t] = Ï„decayu[t âˆ’ 1] + I[t]
4: end for
5: if u[t] < Vth then
6: o[t] = 0
7: else
8: o[t] = 1
9: u[t] = 0
10: end if
11: return o
12: end function
In the Training:
1: Forward:
2: for t = 1 to T do
3: x
1
[t] = W1 ~ X[t] + B1
4: end for
5: y
1 â† tdBN for training(x
1
)
6: \\The training phase of Algorithm 1
7: o
1 â† NeuronU pdate(u
1
, y1
)
8: for i = 2 to N do
9: for t = 1 to T do
10: x
i
[t] = Wi ~ o
iâˆ’1
[t] + Bi
11: end for
12: y
i â† tdBN for training(x
i
)
13: o
i â† NeuronU pdate(u
i
, yi
)
14: end for
15: Q â† Decodinglayer(o
N )
16: L â† ComputeLoss(Y, Q)
17: Backward:
18: âˆ‚L
âˆ‚oi
,
âˆ‚u
âˆ‚L
i â† Autograd
In the Inference:
1: x
1 â† tdBN for inference(W1, B1, X)
2: u
1
, o1 â† NeuronU pdate(u
1
, x1
)
3: for i = 2 to N do
4: x
i â† tdBN for inference(Wi
, Bi
, xiâˆ’1
)
5: \\The inference phase of Algorithm 1
6: o
i â† NeuronU pdate(u
i
, xi
)
7: end for
8: Q â† Decodinglayer(o
N )
where fj,Î¸
t
j
represents the function of j
th block at timestep
t and define its input-output jacobian matrix as âˆ‚f t
âˆ‚f t
j
jâˆ’1
= Jj
t
.
For each block, the second moment of input vector and the
output vector are fixed to Vth
2
, according to Lemma 2, we
have
Ï†(Jj
t
(Jj
t
)
T
) = 1. (20)
So for each time step, according to Lemma 1, SNN with
tdBN at timestep t can achieve â€Block Dynamic Isometryâ€,
which means E[k âˆ†Î¸i
tk 2
2
] at any moment will not increase or
diminish sharply when the SNN goes deep. And the âˆ†Î¸i can
be defined by
âˆ†Î¸i =
T
X
t=1
âˆ†Î¸i
t
. (21)
Under the condition that each âˆ†Î¸i
t
at any timestep t satisfies
the i.i.d assumption and E[âˆ†Î¸i
t
] â‰ˆ 0, we have
E[(âˆ†Î¸i)
2
] â‰ˆ
T
X
t=1
E[(âˆ†Î¸i
t
)
2
]. (22)
So, the total E[k âˆ†Î¸ik
2
2
] will remain stable as well as
E[k âˆ†Î¸i
tk 2
2
], which means the SNN can avoid gradient vanï¿¾ishing or explosion.
Theorem 2. With the iterative LIF model, assuming the preï¿¾activation x
t âˆ¼ N(0, Ïƒin
2
), we have the membrane potential
u
t âˆ¼ N(0, Ïƒout
2
) and Ïƒout
2 âˆ Ïƒin
2
.
Proof. The iterative LIF model can be expressed by
u
t = Ï„decayu
t
(1 âˆ’ o
tâˆ’1
) + x
t
, (23)
o
t =

1
0
if
otherwise
u
t > Vth
, (24)
where u
t
is the membrane potential of the neuron at timestep
t, o
t
is the binary spike and Ï„decay is the potential decay
constant. So considering the membrane potential u
t
at the
moment of t and assuming its last firing time is time t
0 , we
have
u
t =
t X
p=t
0
Ï„decay
tâˆ’p
x
p
, (25)
where x
p denotes the pre-activation at p moment and Ï„decay
represents the decay constant. Because Ï„decay is a relative
tiny constant(e.g. 0.25) in our SNN model, so
u
t â‰ˆ Ï„decayx
tâˆ’1 + x
t
. (26)
Assume each x
t
is i.i.d. sample from N(0, Ïƒin
2
), so the
membrane potential u
t âˆ¼ N(0, Ïƒout
2
) and Ïƒout
2 âˆ Ïƒin
2
. With
Fig. 3 in main paper, we find the high degree of similarity
between the distributions of pre-activations and membrane
potential, which supports the theorem.
Table 4: Network architectures of our ResNet-SNN
17-layers 19-layers 34-layers(standard/large) 50-layers
conv1 3 Ã— 3, 64, s1 3 Ã— 3, 128, s1 7 Ã— 7, 64/128, s2 7 Ã— 7, 64, s2
block1  3
3
Ã—
Ã—
3
3
,
,
64
64 
âˆ—
Ã— 3

3
3
Ã—
Ã—
3
3
,
,
128
128  Ã— 3

3
3
Ã—
Ã—
3
3
,
,
64
64
/
/
128
128 
âˆ—
Ã— 3
 
1
1
3
Ã—
Ã—
Ã—
1
1
3
,
,
,
256
64
64 !
âˆ—
Ã— 3
block2  3
3
Ã—
Ã—
3
3
,
,
128
128 
âˆ—
Ã— 4

3
3
Ã—
Ã—
3
3
,
,
256
256 
âˆ—
Ã— 3

3
3
Ã—
Ã—
3
3
,
,
128
128
/
/
256
256 
âˆ—
Ã— 4
 
1
3
1
Ã—
Ã—
Ã—
1
3
1
,
,
,
128
128
512 !
âˆ—
Ã— 4
block3  3
3
Ã—
Ã—
3
3
,
,
512
512 
âˆ—
Ã— 2

3
3
Ã—
Ã—
3
3
,
,
256
256
/
/
512
512 
âˆ—
Ã— 6
 
1
1
3
Ã—
Ã—
Ã—
1
1
3
,
,
,
1024
256
256 !
âˆ—
Ã— 6
block4  3
3
Ã—
Ã—
3
3
,
,
512
512
/
/
1024
1024 
âˆ—
Ã— 3
 
1
1
3
Ã—
Ã—
Ã—
1
1
3
,
,
,
2048
512
512 !
âˆ—
Ã— 3
average pool/2,256-d fc
11-d fc,softmax 10-d fc,softmax average pool,1000-d fc,softmax
* means the first basic block in the series perform downsampling directly with convolution kernels and a stride of 2.
C Details of Experiments
Network architectures
In our experiment, we test ResNet-19 on CIFAR-10 and
DVS-CIFAR10, ResNet-34 (standard/large) and ResNet-50
on ImageNet, ResNet-17 on DVS-Gessture. The detailed
network architectures are shown in Table 4.
Dataset introduction and pre-processing
CIFAR-10 CIFAR-10 is an image dataset with 50000
training images and 10000 testing images with size of 32 Ã—
32, which all belong to 10 classes. We randomly flip and
crop every image. And every image is normalized by subï¿¾tracting the global mean value of pixel intensity and devï¿¾ided by the standard variance along RGB channels during
the data pre-processing.
ImageNet ImageNet contains 1.28 million training imï¿¾ages and 50000 validating images. Our data pre-processing
uses the usual practice, which randomly crops and flips the
224Ã—224 image with general normalization method as what
we use in CIFAR-10.
DVS-Gesture DVS-Gesture is a collection of moving gesï¿¾tures performed by 29 different individuals, which is capï¿¾tured by DVS cameras under three lighting conditions. It
contains 1342 records from 23 subjects in training set and
288 examples from other 6 subjects in testing data. Belongï¿¾ing to 11 classes, each instance in DVS-Gesture is a stream
of events with size of 128 Ã— 128. In this experiment, we
downsize the event steam to 32 Ã— 32 and sample a slice evï¿¾ery 30ms during both training and testing process. We set
timestep T to be 40. In each time step, the network receives
only one slice, which means the first 1200ms of each action
is used for training or testing.
DVS-CIFAR10 DVS-CIFAR10 is a neuromorphic dataset
converted from famous CIFAR-10 to its dynamic form. It
consists of 1000 images in the format of spike train per class
with total 10 classes. In our experiment, the dataset is split
into a training set with 9000 images and testing set with 1000
images. We downsample the original 128 Ã— 128 image size
to 42 Ã— 42 reduce the temporal resolution by accumulating
the spike train within every 5ms.
Training settings
Optimizer For all of our experiments, we use the stochasï¿¾tic gradient descent (SGD) optimizer with initial learning
rate r = 0.1 and momentum 0.9. For ResNet-19 and
ResNet-34 (standard/large), we let r decay to 0.1r every 35
epoches. For ResNet-50, we let r decay to 0.1r every 45
epoches. For ResNet-17, we let r decay to 0.1r every 1000
epoches.
Acceleration methods All of the models are programmed
on Pytorch. And to accelerate the training process and reï¿¾duce memory cost, we adopt the mixed precision training.
Some models should be trained on multi-GPU, so we use
the sync-BN technique to reduce the influence of small batch
size. The batch sizes of every experiment are shown in Table
5.
Table 5: Batch size in experiments
Network Architecture Batch size per GPU GPU
ResNet-17 40 1
ResNet-19 36 1
ResNet-34(standard) 24 8
ResNet-34(large) 10 8
ResNet-50 10 8
D Analysis of Computation Reduction
Computing patterns of SNNs and ANNs
As we all know, SNN cannot beat ANN on accuracy comï¿¾parisons of most recognition tasks. However, compared with
ANNs, SNNs are much more energy-efficient due to their
binary spikes and event-driven neural operations on the speï¿¾cialized hardware. For ANNs, the activations are determined
by multiply-accumulate (MAC) operations. For SNNs, the
compute cost is mainly the accumulate (AC) operations,
which has two main differences: (1) In the fully-spiking
SNN, there is no multiplication operation because of biï¿¾nary spike inputs. With Eq. (5) and (6), the pre-activations
x
t =
P j wjo
t
(j) and o
t
(j) = 0 or 1, which means x
t
is an accumulation of weights wj in fact. So most comï¿¾pute operations we need in SNNs are additions not the mulï¿¾tiplications. (2) The event-driven operation means that for
each neuron the synaptic computation only occurs when reï¿¾quired. If there is no spike received, there is no need to comï¿¾pute. With these characteristics, despite that SNNs need to
be evaluated over T timesteps, their computation costs are
still lower than ANNs because the AC operations cost less
than MAC operations and the sparse spikes decrease SNNsâ€™
computing operations a lot.
Computation efficiency of SNNs
In Fig. 6, we analyze the average numbers of spikes per neuï¿¾ron in each layer of ResNet-19 on CIFAR-10 and ResNet-
34(large) on ImageNet. With much fewer timesteps, the
spikes are surprisingly sparse. As the network goes deeper,
the firing rates also decrease slowly. With these data, we can
estimate how many AC operations our models need during
a single inference process (ignore the computation of actiï¿¾vation functions in ANNs and membrane potential update in
SNNs). The results are shown in Table 6.
Table 6: Computation comparisons between SNNs and
ANNs
Models Additions Multiplications
SNN ResNet-19 1.8 Ã— 109 3.4 Ã— 107
ResNet-34(large) 1.2 Ã— 1010 1.2 Ã— 109
ANN ResNet-19 2.2 Ã— 109 2.2 Ã— 109
ResNet-34(large) 1.4 Ã— 1010 1.4 Ã— 1010
It should be pointed out that our SNN models need multiï¿¾plications because the first encoding layer converts real imï¿¾ages to spikes and the last decoding layer uses spiking sigï¿¾nals to predict output classes. The comparison shows that
sparse spikes in our SNN models lead to significant comï¿¾putation reduction and energy efficiency. Moreover, a single
feed-forward pass in SNNs implemented in a neuromorphic
architecture might be faster than an ANN implementation
because of event-driven operations. So, with our much fewer
timesteps, our SNNs can also show advantages on running
time over ANNs of same architectures.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17
Layer
0.0
0.5
1.0
1.5
2.0
2.5
3.0
(a) Spiking activities in each layer of ResNet-19
1 2 3 4 5 6 7 8 9 101112131415161718192021222324252627282930313233
Layer
0.0
0.5
1.0
1.5
2.0
2.5
3.0
(b) Spiking activities in each layer of ResNet-34(large)
Figure 6: Spiking activities analysis of ResNet-19 and
ResNet-34(large).
Average Number of Spikes Per Neuron
